{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e06d6e61f4e40a59dc962bb356f1c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d296e59d9cc4669819ca1f7447f9b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108197/108197 [4:38:59<00:00,  6.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "from tqdm import tqdm  # 导入tqdm库\n",
    "\n",
    "# 加载模型 - Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", cache_dir='embedding_model', model_max_length=512)\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", cache_dir='embedding_model')\n",
    "\n",
    "# 平均池化 - Average pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # 获取文本的向量 - Get the vector of the text\n",
    "def get_vector(text):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # 使用平均池化获取文本向量 - Use average pooling to get text vectors\n",
    "    input_ids = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return input_ids\n",
    "\n",
    "# 读取本地数据库并转换为二维向量存储 - Read the local database and convert it to a two-dimensional vector storage\n",
    "def read_and_save(file):\n",
    "    file = json.load(open(file, 'r', encoding='utf-8'))\n",
    "    vectors = []\n",
    "    for i in tqdm(range(len(file))):  # 使用tqdm来包裹循环\n",
    "        line = str(file[i])\n",
    "        vector = get_vector(line)\n",
    "        vectors.append(vector)\n",
    "    vectors = torch.cat(vectors, dim=0)\n",
    "    torch.save(vectors, './local_vectors/vectors.pt') # 保存向量的路径及文件名\n",
    "    print('generate vectors successfully!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_and_save('gushiwen.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T04:30:53.455501Z",
     "start_time": "2023-08-15T04:30:09.405354Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 55\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     54\u001B[0m     input_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m李白的诗歌\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 55\u001B[0m     knowledges \u001B[38;5;241m=\u001B[39m \u001B[43mget_domain_knowledge\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(knowledges, \u001B[38;5;28mlen\u001B[39m(knowledges))\n",
      "Cell \u001B[0;32mIn[1], line 22\u001B[0m, in \u001B[0;36mget_domain_knowledge\u001B[0;34m(text, n, threshold)\u001B[0m\n\u001B[1;32m     20\u001B[0m vectors \u001B[38;5;241m=\u001B[39m read_local_vectors()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# 将输入文本转化为向量 - Convert the input text into a vector\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m input_vector \u001B[38;5;241m=\u001B[39m \u001B[43mget_vector\u001B[49m(text)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 将输入文本转化为numpy数组 - Convert the input text into a numpy array\u001B[39;00m\n\u001B[1;32m     24\u001B[0m input_vector \u001B[38;5;241m=\u001B[39m input_vector\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'get_vector' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "# 读取本地数据库 - Read the local database\n",
    "def read_local_vectors():\n",
    "    vectors = torch.load('./local_vectors/vectors.pt')\n",
    "    return vectors\n",
    "\n",
    "# 将输入文本转化为向量并与数据库中的向量进行比较 - Convert the input text into a vector and compare it with the vector in the database\n",
    "# 输出相似度最高的前n个文本的序号 - Output the serial number of the top 5 texts with the highest similarity\n",
    "def get_domain_knowledge(text, n, threshold=0.2):\n",
    "    # 参数介绍 - Parameter introduction\n",
    "    # text: 输入文本 - Input text\n",
    "    # n: 输出相似度最高的前n个文本的序号 - Output the serial number of the top 5 texts with the highest similarity\n",
    "    # threshold: 概率阈值，小于该阈值的知识将被忽略 - Probability threshold, texts with probability less than this threshold will be ignored\n",
    "    \n",
    "    # 读取数据库中的向量 - Read the vector in the database\n",
    "    vectors = read_local_vectors()\n",
    "    # 将输入文本转化为向量 - Convert the input text into a vector\n",
    "    input_vector = get_vector(text)\n",
    "    # 将输入文本转化为numpy数组 - Convert the input text into a numpy array\n",
    "    input_vector = input_vector.detach().numpy()\n",
    "    # 将数据库中的向量转化为numpy数组 - Convert the vector in the database to a numpy array\n",
    "    vectors = vectors.detach().numpy()\n",
    "    # 计算每个问题与输入文本的相似度 - Calculate the similarity between each question and the input text\n",
    "    similarity = cosine_similarity(input_vector, vectors)\n",
    "    print(similarity)\n",
    "    similarity_sorted = np.squeeze(similarity)\n",
    "    # 按照相似度从大到小排序 - Sort by similarity from large to small\n",
    "    similarity_sorted = np.argsort(-similarity_sorted)\n",
    "    if n > len(similarity):\n",
    "        n = len(similarity)\n",
    "    if len(similarity) > 0:\n",
    "        # 取出相似度最高的前n个文本的序号 - Take out the serial number of the top n texts with the highest similarity\n",
    "        knowledges_ids = similarity_sorted[:n].tolist()\n",
    "        # 读取知识库 - Read the knowledge base\n",
    "        with open('gushiwen.json', 'r', encoding='utf8') as file:\n",
    "            file_content = file.read()\n",
    "            knowledges = json.loads(file_content)\n",
    "\n",
    "            # 去除概率小于阈值的知识 - Remove knowledge with probability less than threshold\n",
    "            knowledges_ids = [i for i in knowledges_ids if similarity[0][i] > threshold]\n",
    "            # 直接输出资料文本 - directly output the text\n",
    "            knowledges = [str(knowledge) for knowledge in knowledges]\n",
    "            # 取出相似度最高的前n个文本 - Take out the top n texts with the highest similarity\n",
    "            knowledges = [knowledges[i] for i in knowledges_ids]\n",
    "        return knowledges\n",
    "    return ''\n",
    "\n",
    "# sample:\n",
    "if __name__ == '__main__':\n",
    "    input_text = '李白的诗歌'\n",
    "    knowledges = get_domain_knowledge(input_text, 5)\n",
    "    print(knowledges, len(knowledges))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
